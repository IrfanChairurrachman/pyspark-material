{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Exploratory Data Analysis**\n",
    "\n",
    "- Part 2 of Large-scale Data Processing with PySpark for DBS\n",
    "- Durasi: 9 jam\n",
    "- *Last Updated*: January 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "\n",
    "Dalam workshop ini, kami akan berfokus pada serangkaian metode pembelajaran *Data Processing with PySpark*. Adapun beberapa *module* yang disediakan antara lain:\n",
    "\n",
    "- **Frequency tables in PySpark SQL**\n",
    "    - Higher dimensional table\n",
    "    - Data aggregation\n",
    "- **Dealing with untidy data**\n",
    "    - Checking NaN values\n",
    "    - Missing values treatment\n",
    "    - Removing duplicates value\n",
    "- **Calculation in PySpark**\n",
    "    - Feature engineering in PySpark\n",
    "    - Extract information from datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Apa itu EDA?\n",
    "Exploratory Data Analysis (EDA) adalah suatu proses untuk melakukan eksplorasi lebih jauh terhadap data, seperti:\n",
    "- melihat struktur data, \n",
    "- melihat sebaran data,\n",
    "- menyesuaikan bentuk tipe data untuk analisis lebih lanjut.\n",
    "\n",
    "Ini juga dapat membantu menentukan apakah teknik statistik yang Anda pertimbangkan untuk analisis data sudah sesuai. Awalnya dikembangkan oleh matematikawan Amerika John Tukey pada 1970-an, teknik EDA terus menjadi metode yang banyak digunakan dalam proses penemuan data saat ini.\n",
    "\n",
    "## Mengapa EDA penting?\n",
    "Tujuan utama EDA adalah untuk membantu melihat data sebelum membuat asumsi apa pun.\n",
    "- Ini dapat membantu mengidentifikasi kesalahan yang jelas,\n",
    "- serta lebih memahami pola dalam data,\n",
    "- mendeteksi outlier atau kejadian anomali,\n",
    "- menemukan hubungan yang menarik antara variabel.\n",
    "\n",
    "Ilmuwan data dan Analis Data dapat menggunakan analisis eksplorasi untuk:\n",
    "- memastikan hasil yang mereka hasilkan valid dan berlaku untuk setiap hasil dan tujuan bisnis yang diinginkan,\n",
    "- membantu pemangku kepentingan dengan mengonfirmasi bahwa mereka mengajukan pertanyaan yang tepat\n",
    "- EDA selesai dan wawasan diambil, fitur-fiturnya kemudian dapat digunakan untuk analisis atau pemodelan data yang lebih canggih, termasuk pembelajaran mesin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "🔻 Anda merupakan seorang data analyst yang diberi sebuah data tabular berformat `.csv`. Anda diminta untuk melakukan eksplorasi terhadap data tersebut hingga mendapatkan insight-insight bisnis yang dapat anda ceritakan kepada orang lain atau rekan Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "🔻 Hal pertama yang harus dilakukan adalah menghubungkan PySpark dengan sumber data yang akan diolah. Dalam hal ini kita tetap menggunakan `loan-missing.csv` berisi informasi peminjaman nasabah pada sebuah bank.\n",
    "\n",
    "Untuk menghubungkan PySpark dengan sumber data, kita akan menggunakan cara yang telah dipelajari di couse sebelumnya mulai dari mengimport `pyspark` hingga pembacaan data menggunakan pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# membuat SparkSession\n",
    "spark = ___\n",
    "\n",
    "# membaca file CSV\n",
    "loan = ___\n",
    "\n",
    "# menampilkan dataframe\n",
    "loan.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔻 Lakukan investigasi awal untuk melihat struktur data terhadap object DataFrame dengan menggunakan method:\n",
    "-  `df.count()` : untuk mendapatkan jumlah baris\n",
    "-  `len(df.columns)`: untuk mendapatkan jumlah kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumlah baris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jumlah kolom\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya perlu dilakukan inspeksi tipe data untuk memastikan tipe data setiap kolomnya telah tepat sehingga akan memudahkan dalam proses lanjutan dalam pengolahan data.\n",
    "\n",
    "💡 Mengecek tipe data di PySpark: `df.printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perubahan Tipe Data (Datetime)\n",
    "\n",
    "Ketika melakukan analisis data, merubah ke tipe data datetime penting dengan manfaat dapat ekstraksi informasi waktu, pengurutan datetime yang akurat serta filtering rentang waktu dengan mudah.\n",
    "\n",
    "Untuk merubah kolom ke tipe data datetime:\n",
    "\n",
    "```python\n",
    "df = df.withColumn('issue_d', to_date(df['date_column'], format='dd/MM/yyyy'))\n",
    "```\n",
    "\n",
    "Notes: parameter `format` mengikuti format kolom tanggal awal.\n",
    "- y = tahun (contoh: 2020; 20)\n",
    "- M = bulan, numerik (contoh: 7; 07)\n",
    "- L = bulan, string (contoh: Jul; July)\n",
    "- d = tanggal (contoh: 28)\n",
    "\n",
    "Simbol lebih lengkap: [Apache Spark Datetime Pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format** yang tepat untuk `issue_d`?\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merubah ke date berdasarkan format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisi Datetime\n",
    "\n",
    "Setelah melakukan konversi tipe data menjadi bentuk `datetime`, kita dapat melakukan partisi untuk menggali informasi yang lebih spesifik seperti tahun, bulan, hari, dan jam. <br>\n",
    "\n",
    "**Date component (numeric):**\n",
    "- `year(df['date_column'])` -> partisi tahun\n",
    "- `month(df['date_column'])` -> partisi bulan\n",
    "- `day(df['date_column'])` -> partisi tanggal\n",
    "- `quarter(df['date_column'])` -> partisi kuarter\n",
    "\n",
    "**Date component (string):**\n",
    "- `date_format(df['date_column'], format)` -> partisi sesuai [format](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. partisi `year(df['date_column'])`**\n",
    "\n",
    "🔻Semisal kita diminta untuk menganalisis karakteristik nasabah berdasarkan **tahun**. Kita dapat membuat kolom baru dengan mengekstrak tahun dari kolom `issue_d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak Tahun\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. partisi `month(df['date_column'])`**\n",
    "\n",
    "🔻Semisal kita diminta untuk menganalisis karakteristik nasabah berdasarkan **bulan**. Kita dapat membuat kolom baru dengan mengekstrak **bulan** dari kolom `issue_d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak Bulan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. partisi `date_format(df['date_column', format])`**\n",
    "\n",
    "🔻Semisal kita diminta untuk membuat kolom baru dengan informasi nama hari seperti \"Thu, 01-12-2011\".\n",
    "\n",
    "Sehingga format yang digunakan adalah `E, dd-MM-yyyy`, dimana:\n",
    "- `E`: nama hari (contoh: Mon, Tue), gunakan format `EEEE` untuk nama hari tidak disingkat\n",
    "- `d`: tanggal\n",
    "- `M`: bulan\n",
    "- `y`: tahun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak Hari\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Data\n",
    "\n",
    "Dalam melakukan analisis data. Terdapat pertanyaan-pertanyaan seperti:\n",
    "\n",
    "- Berapa jumlah customer kita yang memiliki kategori `interest_payment` HIGH?\n",
    "- Berapa rata-rata income untuk setiap kategori di `loan_condition`\n",
    "- Berapa total pinjaman pada tahun 2011?\n",
    "- dan lain-lain.\n",
    "\n",
    "Pertanyaan-pertanyaan tersebut dapat dijawab dengan melakukan **agregasi tabel**. Agregasi tabel adalah tabel hasil pengelompokkan dengan nilai-nilai statistik seperti jumlah, rata-rata, kemunculan, dan lain sebagainya.\n",
    "\n",
    "Untuk melakukan agregasi table di `pyspark` terdapat 2 cara, antara lain:\n",
    "- `df.crosstab()`\n",
    "- `df.groupby()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstab()\n",
    "\n",
    "🔻**Contoh kasus:** sebagai data analis, kita diminta untuk menghitung jumlah customer `BAD LOAN` yang diberikan interest `HIGH`.\n",
    "\n",
    "Untuk menjawab pertanyaan ini, kita dapat menggunakan `.crosstab()`. Fungsi `.crosstab()` di pyspark untuk menghitung jumlah kemunculan dari 2 kolom kategori yang berbeda.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "df.crosstab(\"kategori_1\", \"kategori_2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menjawab pertanyaan tersebut, mari memilih kolom yang diperlukan.\n",
    "> `interest_payment`, `loan_condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat kolom-kolom yang diperlukan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan crosstab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crosstab()` hanya dapat untuk menghitung frekuensi dari 2 kolom kategori. Namun bagaimana jika yang ingin dihitung untuk 1 kategori? atau bahkan menghitung rata-rata nilai numerik dari kolom kategori tertentu?\n",
    "\n",
    "Permasalahan di atas dapat diselesaikan oleh `groupBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk membuat tabel agregasi, kita dapat menggunakan fungsi `groupBy()` diikuti dengan fungsi agregasi yang mau kita hitung.\n",
    "\n",
    "🔻 Sebagai data analis, kita diminta untuk menghitung customer untuk masing-masing kategori di `home_ownership`.\n",
    "\n",
    "**Syntax** `groupBy()`\n",
    "\n",
    "```python\n",
    "df.groupBy('kolom_kategori').FUNC()\n",
    "```\n",
    "\n",
    "Dimana `.FUNC()` bisa berupa:\n",
    "- `.count()`: untuk menghitung jumlah kemunculan\n",
    "- `.sum()`: jumlah semua kolom numerik / `.sum('kolom_numerik')` untuk jumlah kolom tertentu.\n",
    "- `.avg()`: rata-rata semua kolom numerik / `.avg('kolom_numerik')` untuk jumlah kolom tertentu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🔻 Sebagai data analis, kita diminta untuk menghitung rata-rata `loan_amount` dan `annual_inc` untuk customer dengan kondisi loan BAD dan GOOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "\n",
    "Dari hasil di atas, kita akan mendapatkan semua rata-rata dari kolom numerik. Bagaimana jika kita ingin menghitung salah satu kolom saja? semisal rata-rata `annual_inc` untuk customer GOOD loan dan BAD loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg()\n",
    "\n",
    "Misalkan kita ingin membuat tabel agregasi dengan `.FUNC()` yang berbeda-beda untuk masing-masing kolom berupa:\n",
    "- rata-rata untuk `annual_inc`\n",
    "- jumlah untuk `loan_amount` dan `total_pymnt`\n",
    "\n",
    "Untuk mendapatkan hasil tersebut, kita harus melakukan chaining `groupBy` dengan method `agg()`. Kita harus menyertakan mapping (**dictionary**) untuk setiap kolom dengan fungsi agregasinya seperti berikut:\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.agg({\n",
    "    'nama_kolom': 'fungsi_agregasi',\n",
    "    'nama_kolom2': 'fungsi_agregasi'\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untidy Data\n",
    "\n",
    "Dalam melakukan pengolahan data, tidak semua data yang kita miliki adalah data yang \"tidy\". Ada kemungkinan bahwa data kita memiliki nilai kosong (NULL), memiliki nilai yang berulang, dan memiliki nilai yang tidak sesuai dengan nilai variable yang seharusnya (misal usia memiliki nilai minus). Untuk mengatasi hal tersebut, kita dapat melakukan beberapa metode penanganan pada data yang hilang (missing value) atau data yang duplikat (duplicates value). \n",
    "\n",
    "Penanganan data yang hilang atau data yang duplikat menjadi hal yang penting guna memberikan analisa dan insight yang lebih akurat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing Value\n",
    "\n",
    "Untuk mengecek missing values apakah terdapat pada setiap kolom dapat menggunakan 2 cara berikut:\n",
    "- `.inferSchema()`\n",
    "- `df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cara 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat terlihat bahwa semua kolom memiliki missing value. \n",
    "\n",
    "Untuk menghitung berapa jumlah missing value di setiap kolomnya dapat menggunakan cara 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cara 2\n",
    "loan.select([count(when(col(c).isNull(), c)).alias(c) for c in loan.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment Missing Values\n",
    "\n",
    "Beberapa cara umum untuk menangani missing values:\n",
    "\n",
    "1. Hapus baris atau kolom: Menggunakan metode `.dropna()`\n",
    "2. Imputasi nilai NA dengan sebuah nilai\n",
    "3. Tetap mempertahankan missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Values\n",
    "\n",
    "Membuang nilai missing dilakukan dengan fungsi `dropna()`. Secara default, `dropna()` akan menghapus semua baris yang terdapat nilai missingnya. Adapun parameter `.dropna()` antara lain:\n",
    "\n",
    "- `.dropna(how='any')`: hapus baris apabila memiliki **minimal 1 kolom** nilai missing value\n",
    "\n",
    "- `.dropna(how='all')`: harus baris apabila memiliki **semua kolom** nilai missing\n",
    "\n",
    "- `.dropna(thresh=...)`: hapus baris apabila nilai **non-missing** < `thresh` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how=\"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Karena jumlah kolom=29. Sehingga kita ingin minimal 90% (26) data harus terisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh=7 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Terlihat index 5, 7, dan 12 telah terhapus karena jumlah kolom yang notnull di bawah `threshold`. Index 8 masih memiliki missing value tapi tidak di-drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how=\"any\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Terlihat bahwa semua baris yang mengandung missing value akan dihapus. Contoh 5,7,8,12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values\n",
    "\n",
    "Kita akan melakukan imputasi terhadap data yang mengandung missing value, menggunakan metode `.fillna()`\n",
    "\n",
    "💡 **Tips** untuk imputasi:\n",
    "\n",
    "Untuk kolom numerik:\n",
    "\n",
    "- Isi menggunakan pusat data seperti `mean` atau `median`\n",
    "\n",
    "Untuk kolom kategorikal:\n",
    "\n",
    "- Menggunakan `NA` sebagai salah satu dari kategori\n",
    "- Isi menggunakan pusat data (mode)\n",
    "\n",
    "Untuk kolom datetime:\n",
    "\n",
    "- Menggunakan metode [`bfill`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.bfill.html): melakukan imputasi dari baris bawah ke atas\n",
    "- Menggunakan metode [`ffill`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.ffill.html): melakukan imputasi dari baris atas ke bawah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1**\n",
    "Misalkan kita ketahui bahwa nilai missing pada `purpose` menjadi \"other\". `income_category` menjadi \"Low\" serta `income_cat` menjadi 1.0 dengan asumsi customer yang datanya tidak lengkap maka dianggap menjadi berpenghasilan rendah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2** Misalkan kita ingin mengisi **interest_rate** yang NULL dengan nilai rata-rata interest rate dari seluruh customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari rata-rata untuk interest_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi interest rate dengan nilai rata-rata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Opt] Imputer\n",
    "\n",
    "Selain menggunakan cara-cara di atas, pyspark menyediakan metode khusus untuk melakukan imputasi missing value dengan bantuan `Imputer()`. Dengan menggunakan `Imputer()` kita bisa langsung memilih nilai apa yang mau kita hitung dari 3 nilai yaitu **mean, median, dan modus**. Hal yang perlu diperhatikan adalah penggunaan `Imputer()` hanya bisa digunakan pada data atau kolom numerik saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    strategy='mean',\n",
    "    inputCols=['loan_amount'],\n",
    "    outputCols=['loan_amount']\n",
    ")\n",
    "imputer.fit(loan).transform(loan).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kelas ini, kita akan drop semua baris yang memiliki missing values pada data kita. Maka dari itu digunakan `dropna()` dengan nilai default dan akan menyimpan ke dataframe `loan_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_clean = loan.dropna()\n",
    "\n",
    "loan_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan.count() - loan_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat 10rb dari 722rb data yang didrop karena memiliki missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghilangkan duplikat pada dataframe, kita bisa menggunakan fungsi `.dropDuplicates()` atau hanya mengambil nilai uniknya saja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Duplicate\n",
    "\n",
    "Pertama, mari kita lihat baris-baris yang memiliki nilai duplikat di data kita. Untuk melihatnya dapat menggunakan:\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.exceptAll(df.dropDuplicates()).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada data kita masih belum terdapat nilai duplikat. Namun, jika nantinya terdapat nilai duplikat, maka dapat drop nilai duplicated dapat `.dropDuplicates()`.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.dropDuplicates()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi tersebut akan mempertimbangkan semua kolom untuk melihat apakah ada nilai yang duplikat atau tidak. \n",
    "Apabila kita hanya ingin mempertimbangkan kolom tertentu saja maka bisa menambahkan parameter subset\n",
    "\n",
    "**Syntax:**\n",
    "```\n",
    "df.dropDuplicates(subset = ['kolom_1', 'kolom_2'])\n",
    "```\n",
    "\n",
    "**Case:** Semisal kita ingin membuang nilai baris yang kolom `id`, `issue_d`, dan `loan_amount`nya secara bersamaan bernilai duplikat dengan asumsi terdapat duplikat input untuk customer yang sama di hari yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ Warning: Duplikat dapat berarti hal yang berbeda dari sudut pandang data dan sudut pandang analis bisnis. Anda harus ekstra berhati-hati apakah data duplikat memang merupakan karakteristik dari data Anda, atau apakah itu merupakan sebuah kesalahan input data berdasarkan logika bisnisnya."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbs_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
