{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coursebook: Exploratory Data Analysis**\n",
    "\n",
    "- Part 2 of Large-scale Data Processing with PySpark for DBS\n",
    "- Durasi: 9 jam\n",
    "- *Last Updated*: January 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Objectives\n",
    "\n",
    "Dalam workshop ini, kami akan berfokus pada serangkaian metode pembelajaran *Data Processing with PySpark*. Adapun beberapa *module* yang disediakan antara lain:\n",
    "\n",
    "- **Frequency tables in PySpark SQL**\n",
    "    - Higher dimensional table\n",
    "    - Data aggregation\n",
    "- **Dealing with untidy data**\n",
    "    - Checking NaN values\n",
    "    - Missing values treatment\n",
    "    - Removing duplicates value\n",
    "- **Calculation in PySpark**\n",
    "    - Feature engineering in PySpark\n",
    "    - Extract information from datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Apa itu EDA?\n",
    "Exploratory Data Analysis (EDA) adalah suatu proses untuk melakukan eksplorasi lebih jauh terhadap data, seperti:\n",
    "- melihat struktur data, \n",
    "- melihat sebaran data,\n",
    "- menyesuaikan bentuk tipe data untuk analisis lebih lanjut.\n",
    "\n",
    "Ini juga dapat membantu menentukan apakah teknik statistik yang Anda pertimbangkan untuk analisis data sudah sesuai. Awalnya dikembangkan oleh matematikawan Amerika John Tukey pada 1970-an, teknik EDA terus menjadi metode yang banyak digunakan dalam proses penemuan data saat ini.\n",
    "\n",
    "## Mengapa EDA penting?\n",
    "Tujuan utama EDA adalah untuk membantu melihat data sebelum membuat asumsi apa pun.\n",
    "- Ini dapat membantu mengidentifikasi kesalahan yang jelas,\n",
    "- serta lebih memahami pola dalam data,\n",
    "- mendeteksi outlier atau kejadian anomali,\n",
    "- menemukan hubungan yang menarik antara variabel.\n",
    "\n",
    "Ilmuwan data dan Analis Data dapat menggunakan analisis eksplorasi untuk:\n",
    "- memastikan hasil yang mereka hasilkan valid dan berlaku untuk setiap hasil dan tujuan bisnis yang diinginkan,\n",
    "- membantu pemangku kepentingan dengan mengonfirmasi bahwa mereka mengajukan pertanyaan yang tepat\n",
    "- EDA selesai dan wawasan diambil, fitur-fiturnya kemudian dapat digunakan untuk analisis atau pemodelan data yang lebih canggih, termasuk pembelajaran mesin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "ðŸ”» Anda merupakan seorang data analyst yang diberi sebuah data tabular berformat `.csv`. Anda diminta untuk melakukan eksplorasi terhadap data tersebut hingga mendapatkan insight-insight bisnis yang dapat anda ceritakan kepada orang lain atau rekan Anda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "ðŸ”» Hal pertama yang harus dilakukan adalah menghubungkan PySpark dengan sumber data yang akan diolah. Dalam hal ini kita tetap menggunakan `loan-missing.csv` berisi informasi peminjaman nasabah pada sebuah bank.\n",
    "\n",
    "Untuk menghubungkan PySpark dengan sumber data, kita akan menggunakan cara yang telah dipelajari di couse sebelumnya mulai dari mengimport `pyspark` hingga pembacaan data menggunakan pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+----------+--------------+--------------+------------------+---------------+----------+----------+-----------+----------+--------+----------------+------------------+-----------+-----------------+--------------------+--------------+------------------+-------------+-----+---------+-----+-----------------+---------------+----------+-----------+------------+\n",
      "|_c0|       id|  year|   issue_d|emp_length_int|home_ownership|home_ownership_cat|income_category|annual_inc|income_cat|loan_amount|      term|term_cat|application_type|           purpose|purpose_cat|interest_payments|interest_payment_cat|loan_condition|loan_condition_cat|interest_rate|grade|grade_cat|  dti|      total_pymnt|total_rec_prncp|recoveries|installment|      region|\n",
      "+---+---------+------+----------+--------------+--------------+------------------+---------------+----------+----------+-----------+----------+--------+----------------+------------------+-----------+-----------------+--------------------+--------------+------------------+-------------+-----+---------+-----+-----------------+---------------+----------+-----------+------------+\n",
      "|  0|1077501.0|2011.0|01/12/2011|          10.0|          RENT|               1.0|            Low|   24000.0|       1.0|     5000.0| 36 months|     1.0|      INDIVIDUAL|       credit_card|        1.0|              Low|                 1.0|     Good Loan|               0.0|        10.65|    B|      2.0|27.65|      5861.071414|         5000.0|       0.0|     162.87|     munster|\n",
      "|  1|1077430.0|2011.0|01/12/2011|           0.5|          RENT|               1.0|            Low|   30000.0|       1.0|     2500.0| 60 months|     2.0|      INDIVIDUAL|               car|        2.0|             High|                 2.0|      Bad Loan|               1.0|        15.27|    C|      3.0|  1.0|          1008.71|         456.46|    117.08|      59.83|    leinster|\n",
      "|  2|1077175.0|2011.0|01/12/2011|          10.0|          RENT|               1.0|            Low|   12252.0|       1.0|     2400.0| 36 months|     1.0|      INDIVIDUAL|    small_business|        3.0|             High|                 2.0|     Good Loan|               0.0|        15.96|    C|      3.0| 8.72|      3003.653644|         2400.0|       0.0|      84.33|    cannught|\n",
      "|  3|1076863.0|2011.0|01/12/2011|          10.0|          RENT|               1.0|            Low|   49200.0|       1.0|    10000.0| 36 months|     1.0|      INDIVIDUAL|             other|        4.0|             High|                 2.0|     Good Loan|               0.0|        13.49|    C|      3.0| 20.0|      12226.30221|        10000.0|       0.0|     339.31|      ulster|\n",
      "|  4|1075358.0|2011.0|01/12/2011|           1.0|          RENT|               1.0|            Low|   80000.0|       1.0|     3000.0| 60 months|     2.0|      INDIVIDUAL|             other|        4.0|              Low|                 1.0|     Good Loan|               0.0|        12.69|    B|      2.0|17.94|          3242.17|         2233.1|       0.0|      67.79|      ulster|\n",
      "|  5|1075269.0|2011.0|01/12/2011|           3.0|          RENT|               1.0|           NULL|      NULL|      NULL|       NULL|      NULL|    NULL|            NULL|              NULL|       NULL|             NULL|                NULL|          NULL|              NULL|         NULL| NULL|      1.0| 11.2|      5631.377753|         5000.0|       0.0|     156.46|     munster|\n",
      "|  6|1069639.0|2011.0|01/12/2011|           8.0|          RENT|               1.0|            Low|   47004.0|       1.0|     7000.0| 60 months|     2.0|      INDIVIDUAL|debt_consolidation|        6.0|             High|                 2.0|     Good Loan|               0.0|        15.96|    C|      3.0|23.51|          8136.84|        5110.85|       0.0|     170.08|    leinster|\n",
      "|  7|1072053.0|2011.0|01/12/2011|           9.0|          RENT|               1.0|           NULL|      NULL|      NULL|       NULL|      NULL|    NULL|            NULL|              NULL|       NULL|             NULL|                 2.0|     Good Loan|               0.0|        18.64|    E|      5.0| 5.35|3938.144334000001|         3000.0|       0.0|     109.43|      ulster|\n",
      "|  8|1071795.0|2011.0|01/12/2011|           4.0|           OWN|               2.0|           NULL|   40000.0|       1.0|     5600.0|      NULL|     2.0|      INDIVIDUAL|    small_business|        3.0|             High|                 2.0|      Bad Loan|               1.0|        21.28|    F|      6.0| 5.55|           646.02|         162.02|    189.06|     152.39|      ulster|\n",
      "|  9|1071570.0|2011.0|01/12/2011|           0.5|          RENT|               1.0|            Low|   15000.0|       1.0|     5375.0| 60 months|     2.0|      INDIVIDUAL|             other|        4.0|              Low|                 1.0|      Bad Loan|               1.0|        12.69|    B|      2.0|18.08|          1476.19|         673.48|    269.29|     121.45|     munster|\n",
      "| 10|1070078.0|2011.0|01/12/2011|           5.0|           OWN|               2.0|            Low|   72000.0|       1.0|     6500.0| 60 months|     2.0|      INDIVIDUAL|debt_consolidation|        6.0|             High|                 2.0|     Good Loan|               0.0|        14.65|    C|      3.0|16.12|          7677.52|         6500.0|       0.0|     153.45|     munster|\n",
      "| 11|1069908.0|2011.0|01/12/2011|          10.0|           OWN|               2.0|            Low|   75000.0|       1.0|    12000.0| 36 months|     1.0|      INDIVIDUAL|debt_consolidation|        6.0|              Low|                 1.0|     Good Loan|               0.0|        12.69|    B|      2.0|10.78|         13943.08|        12000.0|       0.0|     402.54|      ulster|\n",
      "| 12|1064687.0|2011.0|01/12/2011|           0.5|          RENT|               1.0|           NULL|   30000.0|      NULL|     9000.0|      NULL|     1.0|            NULL|debt_consolidation|       NULL|             High|                NULL|      Bad Loan|              NULL|        13.49| NULL|      3.0| NULL|           2270.7|        1256.14|     444.3|     305.38|    leinster|\n",
      "| 13|1069866.0|2011.0|01/12/2011|           3.0|          RENT|               1.0|            Low|   15000.0|       1.0|     3000.0| 36 months|     1.0|      INDIVIDUAL|       credit_card|        1.0|              Low|                 1.0|     Good Loan|               0.0|         9.91|    B|      2.0|12.56|      3478.981915|         3000.0|       0.0|      96.68|    cannught|\n",
      "| 14|1069057.0|2011.0|01/12/2011|           3.0|          RENT|               1.0|            Low|  100000.0|       1.0|    10000.0| 36 months|     1.0|      INDIVIDUAL|             other|        4.0|              Low|                 1.0|      Bad Loan|               1.0|        10.65|    B|      2.0| 7.06|          7471.99|        5433.47|     645.1|     325.74|      ulster|\n",
      "| 15|1069759.0|2011.0|01/12/2011|           0.5|          RENT|               1.0|            Low|   28000.0|       1.0|     1000.0| 36 months|     1.0|      INDIVIDUAL|debt_consolidation|        6.0|             High|                 2.0|     Good Loan|               0.0|        16.29|    D|      4.0|20.31|      1270.171106|         1000.0|       0.0|      35.31|    cannught|\n",
      "| 16|1065775.0|2011.0|01/12/2011|           4.0|          RENT|               1.0|            Low|   42000.0|       1.0|    10000.0| 36 months|     1.0|      INDIVIDUAL|  home_improvement|        7.0|             High|                 2.0|     Good Loan|               0.0|        15.27|    C|      3.0| 18.6|      12519.26045|        10000.0|       0.0|     347.98|      ulster|\n",
      "| 17|1069971.0|2011.0|01/12/2011|          10.0|      MORTGAGE|               3.0|         Medium|  110000.0|       2.0|     3600.0| 36 months|     1.0|      INDIVIDUAL|    major_purchase|        8.0|              Low|                 1.0|     Good Loan|               0.0|         6.03|    A|      1.0|10.52|          3785.02|         3600.0|       0.0|     109.57|Northern-Irl|\n",
      "| 18|1062474.0|2011.0|01/12/2011|           1.0|      MORTGAGE|               3.0|            Low|   84000.0|       1.0|     6000.0| 36 months|     1.0|      INDIVIDUAL|           medical|        9.0|              Low|                 1.0|     Good Loan|               0.0|        11.71|    B|      2.0|18.44|      7164.499852|         6000.0|       0.0|     198.46|      ulster|\n",
      "| 19|1069742.0|2011.0|01/12/2011|           6.0|          RENT|               1.0|            Low|   77385.0|       1.0|     9200.0| 36 months|     1.0|      INDIVIDUAL|debt_consolidation|        6.0|              Low|                 1.0|     Good Loan|               0.0|         6.03|    A|      1.0| 9.86|          9459.96|         9200.0|       0.0|     280.01|      ulster|\n",
      "+---+---------+------+----------+--------------+--------------+------------------+---------------+----------+----------+-----------+----------+--------+----------------+------------------+-----------+-----------------+--------------------+--------------+------------------+-------------+-----+---------+-----+-----------------+---------------+----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# membuat SparkSession\n",
    "spark =SparkSession.builder.appName(\"DBS Indonesia\").getOrCreate()\n",
    "\n",
    "# membaca file CSV\n",
    "loan = spark.read.csv('data_input/loan-missing.csv', header=True, inferSchema=True)\n",
    "\n",
    "# menampilkan dataframe\n",
    "loan.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”» Lakukan investigasi awal untuk melihat struktur data terhadap object DataFrame dengan menggunakan method:\n",
    "-  `df.count()` : untuk mendapatkan jumlah baris\n",
    "-  `len(df.columns)`: untuk mendapatkan jumlah kolom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722423"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jumlah baris\n",
    "loan.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722423"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternatif\n",
    "loan.repartition(1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "722423"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# alternatif, lebih lama\n",
    "len(loan.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_c0</th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>issue_d</th>\n",
       "      <th>emp_length_int</th>\n",
       "      <th>home_ownership</th>\n",
       "      <th>home_ownership_cat</th>\n",
       "      <th>income_category</th>\n",
       "      <th>annual_inc</th>\n",
       "      <th>income_cat</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>grade_cat</th>\n",
       "      <th>dti</th>\n",
       "      <th>total_pymnt</th>\n",
       "      <th>total_rec_prncp</th>\n",
       "      <th>recoveries</th>\n",
       "      <th>installment</th>\n",
       "      <th>region</th>\n",
       "      <th>year_new</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1077501.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>24000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.65</td>\n",
       "      <td>5861.071414</td>\n",
       "      <td>5000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>162.87</td>\n",
       "      <td>munster</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1077430.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>0.5</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>30000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1008.710000</td>\n",
       "      <td>456.46</td>\n",
       "      <td>117.08</td>\n",
       "      <td>59.83</td>\n",
       "      <td>leinster</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1077175.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>12252.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.72</td>\n",
       "      <td>3003.653644</td>\n",
       "      <td>2400.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>84.33</td>\n",
       "      <td>cannught</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1076863.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>10.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>49200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>C</td>\n",
       "      <td>3.0</td>\n",
       "      <td>20.00</td>\n",
       "      <td>12226.302210</td>\n",
       "      <td>10000.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>339.31</td>\n",
       "      <td>ulster</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1075358.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2011-12-01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RENT</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>B</td>\n",
       "      <td>2.0</td>\n",
       "      <td>17.94</td>\n",
       "      <td>3242.170000</td>\n",
       "      <td>2233.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>67.79</td>\n",
       "      <td>ulster</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   _c0         id    year     issue_d  emp_length_int home_ownership   \n",
       "0    0  1077501.0  2011.0  2011-12-01            10.0           RENT  \\\n",
       "1    1  1077430.0  2011.0  2011-12-01             0.5           RENT   \n",
       "2    2  1077175.0  2011.0  2011-12-01            10.0           RENT   \n",
       "3    3  1076863.0  2011.0  2011-12-01            10.0           RENT   \n",
       "4    4  1075358.0  2011.0  2011-12-01             1.0           RENT   \n",
       "\n",
       "   home_ownership_cat income_category  annual_inc  income_cat  ...  grade   \n",
       "0                 1.0             Low     24000.0         1.0  ...      B  \\\n",
       "1                 1.0             Low     30000.0         1.0  ...      C   \n",
       "2                 1.0             Low     12252.0         1.0  ...      C   \n",
       "3                 1.0             Low     49200.0         1.0  ...      C   \n",
       "4                 1.0             Low     80000.0         1.0  ...      B   \n",
       "\n",
       "  grade_cat    dti   total_pymnt total_rec_prncp  recoveries installment   \n",
       "0       2.0  27.65   5861.071414         5000.00        0.00      162.87  \\\n",
       "1       3.0   1.00   1008.710000          456.46      117.08       59.83   \n",
       "2       3.0   8.72   3003.653644         2400.00        0.00       84.33   \n",
       "3       3.0  20.00  12226.302210        10000.00        0.00      339.31   \n",
       "4       2.0  17.94   3242.170000         2233.10        0.00       67.79   \n",
       "\n",
       "     region year_new  month  \n",
       "0   munster   2011.0   12.0  \n",
       "1  leinster   2011.0   12.0  \n",
       "2  cannught   2011.0   12.0  \n",
       "3    ulster   2011.0   12.0  \n",
       "4    ulster   2011.0   12.0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert to pandas\n",
    "loan_df = loan.toPandas()\n",
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_c0                       int32\n",
       "id                      float64\n",
       "year                    float64\n",
       "issue_d                  object\n",
       "emp_length_int          float64\n",
       "home_ownership           object\n",
       "home_ownership_cat      float64\n",
       "income_category          object\n",
       "annual_inc              float64\n",
       "income_cat              float64\n",
       "loan_amount             float64\n",
       "term                     object\n",
       "term_cat                float64\n",
       "application_type         object\n",
       "purpose                  object\n",
       "purpose_cat             float64\n",
       "interest_payments        object\n",
       "interest_payment_cat    float64\n",
       "loan_condition           object\n",
       "loan_condition_cat      float64\n",
       "interest_rate           float64\n",
       "grade                    object\n",
       "grade_cat               float64\n",
       "dti                     float64\n",
       "total_pymnt             float64\n",
       "total_rec_prncp         float64\n",
       "recoveries              float64\n",
       "installment             float64\n",
       "region                   object\n",
       "year_new                float64\n",
       "month                   float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loan_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jumlah kolom\n",
    "len(loan.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selanjutnya perlu dilakukan inspeksi tipe data untuk memastikan tipe data setiap kolomnya telah tepat sehingga akan memudahkan dalam proses lanjutan dalam pengolahan data.\n",
    "\n",
    "ðŸ’¡ Mengecek tipe data di PySpark: `df.printSchema()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- id: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- issue_d: string (nullable = true)\n",
      " |-- emp_length_int: double (nullable = true)\n",
      " |-- home_ownership: string (nullable = true)\n",
      " |-- home_ownership_cat: double (nullable = true)\n",
      " |-- income_category: string (nullable = true)\n",
      " |-- annual_inc: double (nullable = true)\n",
      " |-- income_cat: double (nullable = true)\n",
      " |-- loan_amount: double (nullable = true)\n",
      " |-- term: string (nullable = true)\n",
      " |-- term_cat: double (nullable = true)\n",
      " |-- application_type: string (nullable = true)\n",
      " |-- purpose: string (nullable = true)\n",
      " |-- purpose_cat: double (nullable = true)\n",
      " |-- interest_payments: string (nullable = true)\n",
      " |-- interest_payment_cat: double (nullable = true)\n",
      " |-- loan_condition: string (nullable = true)\n",
      " |-- loan_condition_cat: double (nullable = true)\n",
      " |-- interest_rate: double (nullable = true)\n",
      " |-- grade: string (nullable = true)\n",
      " |-- grade_cat: double (nullable = true)\n",
      " |-- dti: double (nullable = true)\n",
      " |-- total_pymnt: double (nullable = true)\n",
      " |-- total_rec_prncp: double (nullable = true)\n",
      " |-- recoveries: double (nullable = true)\n",
      " |-- installment: double (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "loan.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perubahan Tipe Data (Datetime)\n",
    "\n",
    "Ketika melakukan analisis data, merubah ke tipe data datetime penting dengan manfaat dapat ekstraksi informasi waktu, pengurutan datetime yang akurat serta filtering rentang waktu dengan mudah.\n",
    "\n",
    "Untuk merubah kolom ke tipe data datetime:\n",
    "\n",
    "```python\n",
    "df = df.withColumn('issue_d', to_date(df['date_column'], format='dd/MM/yyyy'))\n",
    "```\n",
    "\n",
    "Notes: parameter `format` mengikuti format kolom tanggal awal.\n",
    "- y = tahun (contoh: 2020; 20)\n",
    "- M = bulan, numerik (contoh: 7; 07)\n",
    "- L = bulan, string (contoh: Jul; July)\n",
    "- d = tanggal (contoh: 28)\n",
    "\n",
    "Simbol lebih lengkap: [Apache Spark Datetime Pattern](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Format** yang tepat untuk `issue_d`?\n",
    "\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merubah ke date berdasarkan format\n",
    "loan = loan.withColumn('issue_d', to_date('issue_d', format = 'dd/MM/yyyy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partisi Datetime\n",
    "\n",
    "Setelah melakukan konversi tipe data menjadi bentuk `datetime`, kita dapat melakukan partisi untuk menggali informasi yang lebih spesifik seperti tahun, bulan, hari, dan jam. <br>\n",
    "\n",
    "**Date component (numeric):**\n",
    "- `year(df['date_column'])` -> partisi tahun\n",
    "- `month(df['date_column'])` -> partisi bulan\n",
    "- `day(df['date_column'])` -> partisi tanggal\n",
    "- `quarter(df['date_column'])` -> partisi kuarter\n",
    "\n",
    "**Date component (string):**\n",
    "- `date_format(df['date_column'], format)` -> partisi sesuai [format](https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A. partisi `year(df['date_column'])`**\n",
    "\n",
    "ðŸ”»Semisal kita diminta untuk menganalisis karakteristik nasabah berdasarkan **tahun**. Kita dapat membuat kolom baru dengan mengekstrak tahun dari kolom `issue_d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+------+----------+--------------+--------------+------------------+---------------+----------+----------+-----------+----------+--------+----------------+--------------+-----------+-----------------+--------------------+--------------+------------------+-------------+-----+---------+-----+-----------+---------------+----------+-----------+--------+--------+\n",
      "|_c0|       id|  year|   issue_d|emp_length_int|home_ownership|home_ownership_cat|income_category|annual_inc|income_cat|loan_amount|      term|term_cat|application_type|       purpose|purpose_cat|interest_payments|interest_payment_cat|loan_condition|loan_condition_cat|interest_rate|grade|grade_cat|  dti|total_pymnt|total_rec_prncp|recoveries|installment|  region|year_new|\n",
      "+---+---------+------+----------+--------------+--------------+------------------+---------------+----------+----------+-----------+----------+--------+----------------+--------------+-----------+-----------------+--------------------+--------------+------------------+-------------+-----+---------+-----+-----------+---------------+----------+-----------+--------+--------+\n",
      "|  0|1077501.0|2011.0|2011-12-01|          10.0|          RENT|               1.0|            Low|   24000.0|       1.0|     5000.0| 36 months|     1.0|      INDIVIDUAL|   credit_card|        1.0|              Low|                 1.0|     Good Loan|               0.0|        10.65|    B|      2.0|27.65|5861.071414|         5000.0|       0.0|     162.87| munster|    2011|\n",
      "|  1|1077430.0|2011.0|2011-12-01|           0.5|          RENT|               1.0|            Low|   30000.0|       1.0|     2500.0| 60 months|     2.0|      INDIVIDUAL|           car|        2.0|             High|                 2.0|      Bad Loan|               1.0|        15.27|    C|      3.0|  1.0|    1008.71|         456.46|    117.08|      59.83|leinster|    2011|\n",
      "|  2|1077175.0|2011.0|2011-12-01|          10.0|          RENT|               1.0|            Low|   12252.0|       1.0|     2400.0| 36 months|     1.0|      INDIVIDUAL|small_business|        3.0|             High|                 2.0|     Good Loan|               0.0|        15.96|    C|      3.0| 8.72|3003.653644|         2400.0|       0.0|      84.33|cannught|    2011|\n",
      "+---+---------+------+----------+--------------+--------------+------------------+---------------+----------+----------+-----------+----------+--------+----------------+--------------+-----------+-----------------+--------------------+--------------+------------------+-------------+-----+---------+-----+-----------+---------------+----------+-----------+--------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ekstrak Tahun\n",
    "loan = loan.withColumn('year_new', year(loan['issue_d']))\n",
    "loan.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B. partisi `month(df['date_column'])`**\n",
    "\n",
    "ðŸ”»Semisal kita diminta untuk menganalisis karakteristik nasabah berdasarkan **bulan**. Kita dapat membuat kolom baru dengan mengekstrak **bulan** dari kolom `issue_d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ekstrak Bulan\n",
    "loan = loan.withColumn('month', month(loan['issue_d']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C. partisi `date_format(df['date_column', format])`**\n",
    "\n",
    "ðŸ”»Semisal kita diminta untuk membuat kolom baru dengan informasi nama hari seperti \"Thu, 01-12-2011\".\n",
    "\n",
    "Sehingga format yang digunakan adalah `E, dd-MM-yyyy`, dimana:\n",
    "- `E`: nama hari (contoh: Mon, Tue), gunakan format `EEEE` untuk nama hari tidak disingkat\n",
    "- `d`: tanggal\n",
    "- `M`: bulan\n",
    "- `y`: tahun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|date_issue_new            |\n",
      "+--------------------------+\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "|Thursday, 01/December/2011|\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ekstrak Hari\n",
    "loan.withColumn('date_issue_new', date_format(loan['issue_d'], format='EEEE, dd/MMMM/yyyy')).\\\n",
    "select('date_issue_new').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END OF DAY 2**\n",
    "\n",
    "---\n",
    "\n",
    "**START OF DAY 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Data\n",
    "\n",
    "Dalam melakukan analisis data. Terdapat pertanyaan-pertanyaan seperti:\n",
    "\n",
    "- Berapa jumlah customer kita yang memiliki kategori `interest_payment` HIGH?\n",
    "- Berapa rata-rata income untuk setiap kategori di `loan_condition`\n",
    "- Berapa total pinjaman pada tahun 2011?\n",
    "- dan lain-lain.\n",
    "\n",
    "Pertanyaan-pertanyaan tersebut dapat dijawab dengan melakukan **agregasi tabel**. Agregasi tabel adalah tabel hasil pengelompokkan dengan nilai-nilai statistik seperti jumlah, rata-rata, kemunculan, dan lain sebagainya.\n",
    "\n",
    "Untuk melakukan agregasi table di `pyspark` terdapat 2 cara, antara lain:\n",
    "- `df.crosstab()`\n",
    "- `df.groupby()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstab()\n",
    "\n",
    "ðŸ”»**Contoh kasus:** sebagai data analis, kita diminta untuk menghitung jumlah customer `BAD LOAN` yang diberikan interest `HIGH`.\n",
    "\n",
    "Untuk menjawab pertanyaan ini, kita dapat menggunakan `.crosstab()`. Fungsi `.crosstab()` di pyspark untuk menghitung jumlah kemunculan dari 2 kolom kategori yang berbeda.\n",
    "\n",
    "**Syntax:**\n",
    "\n",
    "```python\n",
    "df.crosstab(\"kategori_1\", \"kategori_2\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menjawab pertanyaan tersebut, mari memilih kolom yang diperlukan.\n",
    "> `interest_payment`, `loan_condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melihat kolom-kolom yang diperlukan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melakukan crosstab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`crosstab()` hanya dapat untuk menghitung frekuensi dari 2 kolom kategori. Namun bagaimana jika yang ingin dihitung untuk 1 kategori? atau bahkan menghitung rata-rata nilai numerik dari kolom kategori tertentu?\n",
    "\n",
    "Permasalahan di atas dapat diselesaikan oleh `groupBy()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk membuat tabel agregasi, kita dapat menggunakan fungsi `groupBy()` diikuti dengan fungsi agregasi yang mau kita hitung.\n",
    "\n",
    "ðŸ”» Sebagai data analis, kita diminta untuk menghitung customer untuk masing-masing kategori di `home_ownership`.\n",
    "\n",
    "**Syntax** `groupBy()`\n",
    "\n",
    "```python\n",
    "df.groupBy('kolom_kategori').FUNC()\n",
    "```\n",
    "\n",
    "Dimana `.FUNC()` bisa berupa:\n",
    "- `.count()`: untuk menghitung jumlah kemunculan\n",
    "- `.sum()`: jumlah semua kolom numerik / `.sum('kolom_numerik')` untuk jumlah kolom tertentu.\n",
    "- `.avg()`: rata-rata semua kolom numerik / `.avg('kolom_numerik')` untuk jumlah kolom tertentu.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ”» Sebagai data analis, kita diminta untuk menghitung rata-rata `loan_amount` dan `annual_inc` untuk customer dengan kondisi loan BAD dan GOOD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insight:**\n",
    "\n",
    "Dari hasil di atas, kita akan mendapatkan semua rata-rata dari kolom numerik. Bagaimana jika kita ingin menghitung salah satu kolom saja? semisal rata-rata `annual_inc` untuk customer GOOD loan dan BAD loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg()\n",
    "\n",
    "Misalkan kita ingin membuat tabel agregasi dengan `.FUNC()` yang berbeda-beda untuk masing-masing kolom berupa:\n",
    "- rata-rata untuk `annual_inc`\n",
    "- jumlah untuk `loan_amount` dan `total_pymnt`\n",
    "\n",
    "Untuk mendapatkan hasil tersebut, kita harus melakukan chaining `groupBy` dengan method `agg()`. Kita harus menyertakan mapping (**dictionary**) untuk setiap kolom dengan fungsi agregasinya seperti berikut:\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.agg({\n",
    "    'nama_kolom': 'fungsi_agregasi',\n",
    "    'nama_kolom2': 'fungsi_agregasi'\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸ§  Dive Deeper**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Berapakah rata-rata `installment` untuk masing-masing `purpose` pinjaman?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Berapakah rata-rata `annual_inc` untuk setiap `income_category` dan jenis `home_ownership`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Untidy Data\n",
    "\n",
    "Dalam melakukan pengolahan data, tidak semua data yang kita miliki adalah data yang \"tidy\". Ada kemungkinan bahwa data kita memiliki nilai kosong (NULL), memiliki nilai yang berulang, dan memiliki nilai yang tidak sesuai dengan nilai variable yang seharusnya (misal usia memiliki nilai minus). Untuk mengatasi hal tersebut, kita dapat melakukan beberapa metode penanganan pada data yang hilang (missing value) atau data yang duplikat (duplicates value). \n",
    "\n",
    "Penanganan data yang hilang atau data yang duplikat menjadi hal yang penting guna memberikan analisa dan insight yang lebih akurat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inconsistency Values\n",
    "\n",
    "Realitanya, ketika kita melakukan proses pengolahan data, kendala yang sering sekali di hadapi adalah value yang tidak konsisten dalam data. Tidak konsisten yang dimaksudkan disini adalah, terdapat beberapa karakter atau spasi berlebih dan juga penggunaan huruf *lowercase* dan *uppercase* yang tidak sesuai. Pada tahapan ini, kita akan mencoba untuk melakukan proses handling pada data yang tidak konsisten tersebut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Membuat Dummy Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [([\" Dyah Nurlita \",'1994/12/30']), ([\" Samuel   Chan \",'1995/11/30']), ([\"Irfan Rahman\",'1992/11/14'])]\n",
    "columns = [\"Name\",\"Date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('Date', to_date('Date', format='dd/MM/yyyy'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Menghilangkan Spasi Berlebih"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Menghilangkan spasi berlebih yang terdapat di awal dan di akhir kalimat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Menghilangkan spasi di semua text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Mengganti spasi dengan karakter tertentu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Uppercase* dan *Lowercase*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Menggubah text menjadi *uppercase***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mengubah text menjadi *lowercase***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Missing Value\n",
    "\n",
    "Untuk mengecek missing values apakah terdapat pada setiap kolom dapat menggunakan 2 cara berikut:\n",
    "- `df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dapat terlihat bahwa semua kolom memiliki missing value. \n",
    "\n",
    "Untuk menghitung berapa jumlah missing value di setiap kolomnya dapat menggunakan cara 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cara hapus missing values\n",
    "loan.select([count(when(col(c).isNull(), c)).alias(c) for c in loan.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treatment Missing Values\n",
    "\n",
    "Beberapa cara umum untuk menangani missing values:\n",
    "\n",
    "1. Hapus baris atau kolom: Menggunakan metode `.dropna()`\n",
    "2. Imputasi nilai NA dengan sebuah nilai\n",
    "3. Tetap mempertahankan missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Missing Values\n",
    "\n",
    "Membuang nilai missing dilakukan dengan fungsi `dropna()`. Secara default, `dropna()` akan menghapus semua baris yang terdapat nilai missingnya. Adapun parameter `.dropna()` antara lain:\n",
    "\n",
    "- `.dropna(how='any')`: hapus baris apabila memiliki **minimal 1 kolom** nilai missing value\n",
    "\n",
    "- `.dropna(how='all')`: harus baris apabila memiliki **semua kolom** nilai missing\n",
    "\n",
    "- `.dropna(thresh=...)`: hapus baris apabila nilai **non-missing** < `thresh` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how=\"all\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tresh** : Paling tidak ada jumlah kolom sebanyak `thresh` yang terisi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# thresh=28\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Terlihat index 5, 7, dan 12 telah terhapus karena jumlah kolom yang notnull di bawah `threshold`. Index 8 masih memiliki missing value tapi tidak di-drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how=\"any\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Terlihat bahwa semua baris yang mengandung missing value akan dihapus. Contoh 5,7,8,12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute Missing Values\n",
    "\n",
    "Kita akan melakukan imputasi terhadap data yang mengandung missing value, menggunakan metode `.fillna()`\n",
    "\n",
    "ðŸ’¡ **Tips** untuk imputasi:\n",
    "\n",
    "Untuk kolom numerik:\n",
    "\n",
    "- Isi menggunakan pusat data seperti `mean` atau `median`\n",
    "\n",
    "Untuk kolom kategorikal:\n",
    "\n",
    "- Menggunakan `NA` sebagai salah satu dari kategori\n",
    "- Isi menggunakan pusat data (mode)\n",
    "\n",
    "Untuk kolom datetime:\n",
    "\n",
    "- Menggunakan metode [`bfill`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.bfill.html): melakukan imputasi dari baris bawah ke atas\n",
    "- Menggunakan metode [`ffill`](https://spark.apache.org/docs/latest/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.ffill.html): melakukan imputasi dari baris atas ke bawah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 1**\n",
    "Misalkan kita ketahui bahwa nilai missing pada `purpose` menjadi \"other\". `income_category` menjadi \"Low\" serta `income_cat` menjadi 1.0 dengan asumsi customer yang datanya tidak lengkap maka dianggap menjadi berpenghasilan rendah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Case 2** Misalkan kita ingin mengisi **interest_rate** yang NULL dengan nilai rata-rata interest rate dari seluruh customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mencari rata-rata untuk interest_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengisi interest rate dengan nilai rata-rata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Opt] Imputer\n",
    "\n",
    "Selain menggunakan cara-cara di atas, pyspark menyediakan metode khusus untuk melakukan imputasi missing value dengan bantuan `Imputer()`. Dengan menggunakan `Imputer()` kita bisa langsung memilih nilai apa yang mau kita hitung dari 3 nilai yaitu **mean, median, dan modus**. Hal yang perlu diperhatikan adalah penggunaan `Imputer()` hanya bisa digunakan pada data atau kolom numerik saja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer(\n",
    "    strategy='mean',\n",
    "    inputCols=['loan_amount'],\n",
    "    outputCols=['loan_amount']\n",
    ")\n",
    "imputer.fit(loan).transform(loan).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada kelas ini, kita akan drop semua baris yang memiliki missing values pada data kita. Maka dari itu digunakan `dropna()` dengan nilai default dan akan menyimpan ke dataframe `loan_clean`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_clean = loan.dropna()\n",
    "\n",
    "loan_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan.count() - loan_clean.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terdapat 10rb dari 722rb data yang didrop karena memiliki missing value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Duplicated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Untuk menghilangkan duplikat pada dataframe, kita bisa menggunakan fungsi `.dropDuplicates()` atau hanya mengambil nilai uniknya saja.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Duplicate\n",
    "\n",
    "Pertama, mari kita lihat baris-baris yang memiliki nilai duplikat di data kita. Untuk melihatnya dapat menggunakan:\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.exceptAll(df.dropDuplicates()).show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pada data kita masih belum terdapat nilai duplikat. Namun, jika nantinya terdapat nilai duplikat, maka dapat drop nilai duplicated dapat `.dropDuplicates()`.\n",
    "\n",
    "**Syntax:**\n",
    "```python\n",
    "df.dropDuplicates()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fungsi tersebut akan mempertimbangkan semua kolom untuk melihat apakah ada nilai yang duplikat atau tidak. \n",
    "Apabila kita hanya ingin mempertimbangkan kolom tertentu saja maka bisa menambahkan parameter subset\n",
    "\n",
    "**Syntax:**\n",
    "```\n",
    "df.dropDuplicates(subset = ['kolom_1', 'kolom_2'])\n",
    "```\n",
    "\n",
    "**Case:** Semisal kita ingin membuang nilai baris yang kolom `id`, `issue_d`, dan `loan_amount`nya secara bersamaan bernilai duplikat dengan asumsi terdapat duplikat input untuk customer yang sama di hari yang sama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ Warning: Duplikat dapat berarti hal yang berbeda dari sudut pandang data dan sudut pandang analis bisnis. Anda harus ekstra berhati-hati apakah data duplikat memang merupakan karakteristik dari data Anda, atau apakah itu merupakan sebuah kesalahan input data berdasarkan logika bisnisnya."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          Name|\n",
      "+--------------+\n",
      "|  Dyah Nurlita|\n",
      "| Triani Narita|\n",
      "|Victor Nugraha|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_new = [([\"Dyah Nurlita\"]), ([\"Triani Narita\"]), ([\"Victor Nugraha\"])]\n",
    "columns = [\"Name\"]\n",
    "df_new = spark.createDataFrame(data_new, columns)\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Mengambil 4 karakter pertama dari kolom Nama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+\n",
      "|          Name|First_4_Chars|\n",
      "+--------------+-------------+\n",
      "|  Dyah Nurlita|         Dyah|\n",
      "| Triani Narita|         Tria|\n",
      "|Victor Nugraha|         Vict|\n",
      "+--------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.withColumn('First_4_Chars', col(\"Name\").substr(1, 4))\n",
    "\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Mengambil 4 karakter terakhir dari kolom Nama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------+------------+\n",
      "|          Name|First_4_Chars|Last_4_Chars|\n",
      "+--------------+-------------+------------+\n",
      "|  Dyah Nurlita|         Dyah|        lita|\n",
      "| Triani Narita|         Tria|        rita|\n",
      "|Victor Nugraha|         Vict|        raha|\n",
      "+--------------+-------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new = df_new.withColumn('Last_4_Chars', expr(\"substr(Name, length(Name)-3, 4)\"))\n",
    "df_new.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inclass Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+\n",
      "|kolom1|kolom2|kolom3|\n",
      "+------+------+------+\n",
      "|     1|     2|     3|\n",
      "|     4|     5|     6|\n",
      "+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trial = spark.read.csv('data_input/trial.txt', sep = ';',header=True, inferSchema=True)\n",
    "\n",
    "trial.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dbs_pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
